# CodeCrucible Synth Environment Configuration
# Copy this file to .env and fill in your actual values

# Smithery API Configuration
SMITHERY_API_KEY=your_smithery_api_key_here

# MCP Server Configuration  
MCP_TERMINAL_API_KEY=your_terminal_controller_api_key_here
MCP_TASK_MANAGER_API_KEY=your_task_manager_api_key_here

# AI Provider Endpoints (configurable)
OLLAMA_ENDPOINT=http://localhost:11434
LM_STUDIO_ENDPOINT=http://localhost:1234

# E2B Configuration (for secure code execution)
E2B_API_KEY=your_e2b_api_key_here

# ADAPTIVE SLIDING CONTEXT WINDOW CONFIGURATION
MODEL_MAX_CONTEXT_WINDOW=131072  # Maximum context (slides down based on memory)
MODEL_MAX_TOKENS=131072          # Align with context window
MODEL_TEMPERATURE=0.1            # Low temperature for precise function calls
MODEL_DEFAULT_NAME=llama3.1:8b   # Best function calling model
MODEL_DEFAULT_PROVIDER=ollama
MODEL_TOP_P=0.9
MODEL_TOP_K=40
MODEL_BASE_CONFIDENCE=0.9

# Memory Management Configuration
MEMORY_THRESHOLD=0.8             # Use 80% of available GPU memory
ADAPTIVE_CONTEXT_ENABLED=true    # Enable sliding context algorithm
MEMORY_MONITORING_ENABLED=true   # Enable real-time memory monitoring
AUTO_REDUCE_ON_OOM=true          # Automatically reduce context on out-of-memory
CONTEXT_REDUCTION_FACTOR=0.75    # Reduce by 25% on memory pressure

# Context Sliding Configuration
CONTEXT_TIERS="131072,96000,64000,48000,32000,24000,16000,12000,8192"
FALLBACK_CONTEXT=8192            # Minimum safe context
CONTEXT_SLIDING_ENABLED=true     # Enable sliding window algorithm

# OLLAMA MEMORY-AWARE CONFIGURATION
OLLAMA_NUM_CTX=32000             # Start with safe context (will be adapted up)
OLLAMA_NUM_GPU=33                # Maximum GPU layers (will adapt down if needed)
OLLAMA_LOW_VRAM=false            # Auto-enabled if memory pressure detected
OLLAMA_CONNECTION_TIMEOUT=5000
OLLAMA_COLD_START_TIMEOUT=60000
OLLAMA_GENERATION_TIMEOUT=180000 # Increased for sliding context operations
OLLAMA_HEALTH_CHECK_TIMEOUT=3000
OLLAMA_KEEP_ALIVE=15m            # Keep models loaded longer

# GPU Memory Management
GPU_MEMORY_LIMIT=10.7            # Available GPU memory in GB (RTX 4070 SUPER)
GPU_MEMORY_THRESHOLD=0.8         # Use 80% of available memory
GPU_LAYER_FALLBACK="33,28,24,20,16,0"  # Sliding GPU layer fallback
ENABLE_CPU_FALLBACK=true         # Enable CPU fallback if GPU insufficient

# ADAPTIVE TIMEOUT CONFIGURATION (in milliseconds)
REQUEST_TIMEOUT=60000            # Increased for sliding context operations
OLLAMA_TIMEOUT=180000            # Increased for large context processing
LM_STUDIO_TIMEOUT=180000
TOOL_EXECUTION_TIMEOUT=45000     # Increased for memory-aware operations
MEMORY_MONITORING_INTERVAL=30000 # Check memory every 30s
CACHE_TTL=1800000               # 30 minutes cache TTL
CONTEXT_ADAPTATION_TIMEOUT=10000 # Time to adapt context size
JSON_PARSE_TIMEOUT=5000          # Timeout for JSON parsing operations

# Server Configuration
SERVER_PORT=3002
INTERNAL_API_PORT=3000

# MEMORY-AWARE PERFORMANCE CONFIGURATION  
MAX_CONCURRENT_REQUESTS=2        # Reduced to prevent memory contention
RESPONSE_CACHE_SIZE=50           # Reduced cache size to save memory
MEMORY_WARNING_THRESHOLD=0.8     # Memory usage threshold (80%)
MEMORY_CRITICAL_THRESHOLD=0.9    # Critical memory threshold (90%)
MEMORY_CHECK_INTERVAL=30000      # Check memory every 30 seconds
AUTO_CLEANUP_INTERVAL=60000      # Cleanup unused resources every 60s

# Adaptive Performance Settings
ADAPTIVE_BATCH_SIZE=true         # Adjust batch size based on memory
ADAPTIVE_MODEL_SELECTION=true    # Choose models based on available memory
AUTO_UNLOAD_UNUSED_MODELS=true   # Unload models when memory is low
CONTEXT_COMPRESSION_ENABLED=true # Enable context compression when needed

# Security Configuration
STRICT_MODE=true
VALIDATE_INPUTS=true
ENABLE_RATE_LIMITING=true

# Development Settings
NODE_ENV=development
LOG_LEVEL=info
DEBUG_MODE=false

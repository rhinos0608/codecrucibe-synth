
hybrid:
  enabled: true
  routing:
    defaultProvider: "auto"
    escalationThreshold: 0.7
    confidenceScoring: true
    learningEnabled: true
    
  lmStudio:
    endpoint: "http://localhost:1234"
    enabled: true
    models:
      - "codellama-7b-instruct"
      - "gemma-2b-it"
    taskTypes: ["template", "edit", "format", "boilerplate"]
    streamingEnabled: true
    maxConcurrent: 3
    
  ollama:
    endpoint: "http://localhost:11434"
    enabled: true
    models:
      - "codellama:34b"  # Now enabled for 12GB VRAM
      - "gemma:latest"
    taskTypes: ["analysis", "planning", "complex", "multi-file"]
    maxConcurrent: 1
    gpu:
      enabled: true
      layers: -1  # Use all GPU layers
      memory_fraction: 0.85
      
  performance:
    cacheEnabled: true
    metricsCollection: true
    autoOptimization: true
    vramOptimization: true

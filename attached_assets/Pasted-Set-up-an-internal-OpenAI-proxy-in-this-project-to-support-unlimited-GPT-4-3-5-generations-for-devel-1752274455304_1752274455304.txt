Set up an internal OpenAI proxy in this project to support unlimited GPT-4/3.5 generations for developers while separating this from the eventual user-facing version.

Architecture:

Backend: Express with TypeScript

Frontend: React 18 with Vite

Routing: Wouter

Styling: Tailwind + Shadcn

API: RESTful

Add .env support using process.env.OPENAI_API_KEY

Instructions:

Create a utility file at src/lib/openai.ts with a callOpenAI(prompt: string, model?: string) function that:

Sends POST to https://api.openai.com/v1/chat/completions

Uses gpt-4 by default

Uses env key OPENAI_API_KEY

If in dev mode (NODE_ENV=development) and no key exists, return mock data

Create an Express route at src/routes/api/openai.ts:

Route: POST /api/openai

Body: { prompt: string, model?: string }

Responds with { result: string }

Uses the util function

Gracefully handles errors

Wire it into the main server (src/server.ts or index.ts) using app.use("/api/openai", openaiRouter)

Update the .env.example file to include OPENAI_API_KEY=your-key-here

📁 File Structure Summary
pgsql
Copy
Edit
src/
├── lib/
│   └── openai.ts
├── routes/
│   └── api/
│       └── openai.ts
└── server.ts (or index.ts)
.env
.env.example
💻 Full Code Replit AI Should Generate
src/lib/openai.ts
ts
Copy
Edit
export async function callOpenAI(prompt: string, model = "gpt-4"): Promise<string> {
  if (
    process.env.NODE_ENV === "development" &&
    !process.env.OPENAI_API_KEY
  ) {
    return `[DevMode] You asked: "${prompt}" — GPT says: "Keep going, legend."`;
  }

  const res = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model,
      messages: [{ role: "user", content: prompt }],
      temperature: 0.7,
      max_tokens: 1024,
    }),
  });

  const data = await res.json();
  if (!res.ok) throw new Error(data?.error?.message || "OpenAI API Error");

  return data.choices?.[0]?.message?.content ?? "⚠️ No content returned.";
}
src/routes/api/openai.ts
ts
Copy
Edit
import express from "express";
import { callOpenAI } from "../../../lib/openai";

const router = express.Router();

router.post("/", async (req, res) => {
  const { prompt, model } = req.body;

  if (!prompt || typeof prompt !== "string") {
    return res.status(400).json({ error: "Missing or invalid prompt." });
  }

  try {
    const response = await callOpenAI(prompt, model);
    res.json({ result: response });
  } catch (err) {
    console.error("OpenAI proxy error:", err);
    res.status(500).json({ error: "Failed to fetch OpenAI response." });
  }
});

export default router;
src/server.ts or src/index.ts (whichever exists)
Add this import and mount:

ts
Copy
Edit
import openaiRouter from "./routes/api/openai";
app.use("/api/openai", openaiRouter);
.env.example
env
Copy
Edit
# OpenAI Dev Proxy
OPENAI_API_KEY=your-key-here
✅ Now You Can Hit:
ts
Copy
Edit
// Example frontend call (e.g. in useCodeGeneration.ts)
const res = await fetch("/api/openai", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({ prompt: "Explain closures in JS", model: "gpt-4" })
});
const { result } = await res.json();
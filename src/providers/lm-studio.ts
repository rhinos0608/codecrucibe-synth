import axios, { AxiosInstance } from 'axios';
import { logger } from '../core/logger.js';

export interface LMStudioConfig {
  endpoint?: string;
  model?: string;
  timeout?: number;
  apiKey?: string;
}

export class LMStudioProvider {
  private httpClient: AxiosInstance;
  private config: LMStudioConfig;
  private model: string;
  private isAvailable: boolean = false;

  constructor(config: LMStudioConfig) {
    this.config = {
      endpoint: config.endpoint || 'http://localhost:1234',
      model: config.model || 'auto',
      timeout: config.timeout || 30000,
    };

    this.model = this.config.model;

    this.httpClient = axios.create({
      baseURL: this.config.endpoint,
<<<<<<< HEAD
      timeout: 0, // Disable default timeout, use AbortController for context-aware timeouts
=======
      timeout: this.config.timeout,
>>>>>>> 312cb1b60a67735101a751485e0debd903886729
      headers: {
        'Content-Type': 'application/json',
        ...(config.apiKey && { Authorization: `Bearer ${config.apiKey}` }),
      },
    });
  }

  async processRequest(request: any, _context?: any): Promise<any> {
    // Check status first
    if (!this.isAvailable) {
      const available = await this.checkStatus();
      if (!available) {
        throw new Error('LM Studio service is not available');
      }
    }

    return this.generate(request);
  }

  async generate(request: any): Promise<any> {
    try {
      // Get available models first if model is 'auto'
      if (this.model === 'auto') {
        await this.selectOptimalModel();
      }

<<<<<<< HEAD
      // Context-aware timeout calculation
      const abortController = new AbortController();
      const contextTimeout = this.getContextAwareTimeout(request);
      
      const timeoutId = setTimeout(() => {
        console.log(`🏛️ DEBUG: LM Studio context timeout (${Math.round(contextTimeout/1000)}s), aborting...`);
        abortController.abort('context_timeout');
      }, contextTimeout);

      console.log(`🏛️ DEBUG: LM Studio context-aware timeout: ${Math.round(contextTimeout/1000)}s for ${this.estimateTokens(request)} tokens`);

=======
>>>>>>> 312cb1b60a67735101a751485e0debd903886729
      const response = await this.httpClient.post('/v1/chat/completions', {
        model: this.model,
        messages: [
          {
            role: 'system',
            content: `You are the AUDITOR component of CodeCrucible Synth's dual-agent system. Your role is to review, validate, and enhance responses generated by the Ollama model.

AUDITOR RESPONSIBILITIES:
1. **Quality Control**: Review generated code for bugs, security issues, performance problems
2. **Best Practices**: Ensure code follows industry standards and conventions
3. **Completeness Check**: Verify the response fully addresses the user's request
4. **Tool Usage Validation**: Confirm proper use of filesystem tools and codebase interaction
5. **Enhancement**: Suggest improvements, missing edge cases, or better approaches

AUDIT PROCESS:
- Analyze the primary response from Ollama
- Use tools to verify code correctness when needed
- Check for security vulnerabilities and performance issues
- Validate that filesystem operations were used appropriately
- Provide constructive feedback and improvements

AUDIT OUTPUT FORMAT:
- Identify what was done well
- Highlight any issues found
- Suggest specific improvements
- Validate tool usage was appropriate for the task

Your audit should be thorough but concise, focusing on actionable feedback to enhance the final response quality.`,
          },
          {
            role: 'user',
            content: request.prompt || request.text || request.content,
          },
        ],
        tools: request.tools || [],
        temperature: request.temperature || 0.7,
        max_tokens: request.maxTokens || request.max_tokens || 16384,
        stream: false,
<<<<<<< HEAD
        signal: abortController.signal,
      });

      clearTimeout(timeoutId);

=======
      });

>>>>>>> 312cb1b60a67735101a751485e0debd903886729
      const choice = response.data.choices?.[0];
      if (!choice) {
        throw new Error('No response choices returned from LM Studio');
      }

      return {
        content: choice.message?.content || choice.text || '',
        model: this.model,
        provider: 'lm-studio',
        metadata: {
          tokens: response.data.usage?.total_tokens || 0,
          latency: Date.now(), // Will be calculated by caller
          quality: 0.85, // LM Studio generally provides good quality
        },
        usage: {
          totalTokens: response.data.usage?.total_tokens || 0,
          promptTokens: response.data.usage?.prompt_tokens || 0,
          completionTokens: response.data.usage?.completion_tokens || 0,
        },
        finished: choice.finish_reason === 'stop',
      };
    } catch (error: unknown) {
      if (error instanceof Error && 'code' in error && error.code === 'ECONNREFUSED') {
        this.isAvailable = false;
        throw new Error(
          'LM Studio server is not running. Please start LM Studio and enable the local server.'
        );
      }
      logger.error('LM Studio generation failed:', (error as Error).message);
      throw new Error(`LM Studio generation failed: ${(error as Error).message}`);
    }
  }

  async checkStatus(): Promise<boolean> {
    try {
      const response = await this.httpClient.get('/v1/models', { timeout: 5000 });
      this.isAvailable = response.status === 200;

      if (this.isAvailable && response.data.data) {
        const models = response.data.data.map((m: Record<string, unknown>) => m.id);
        logger.info(`LM Studio available with ${models.length} models:`, models.slice(0, 3));

        // Auto-select first available model if not specified
        if (this.model === 'auto' && models.length > 0) {
          this.model = models[0];
          logger.info(`Auto-selected LM Studio model: ${this.model}`);
        }
      }

      return this.isAvailable;
    } catch (error) {
      this.isAvailable = false;
      if (error.code !== 'ECONNREFUSED') {
        logger.warn('LM Studio status check failed:', error.message);
      }
      return false;
    }
  }

  async listModels(): Promise<string[]> {
    try {
      const response = await this.httpClient.get('/v1/models');
      if (response.data.data) {
        return response.data.data.map((m: Record<string, unknown>) => m.id);
      }
      return [];
    } catch (error) {
      logger.error('Failed to list LM Studio models:', error);
      return [];
    }
  }

  private async selectOptimalModel(): Promise<void> {
    try {
      const models = await this.listModels();
      if (models.length === 0) {
        throw new Error('No models available in LM Studio');
      }

      // Prefer fast, efficient models for LM Studio
      const preferredModels = ['tinyllama', 'phi-2', 'codellama-7b', 'mistral-7b', 'zephyr-7b'];

      let selectedModel = models[0]; // fallback

      for (const preferred of preferredModels) {
        const match = models.find(m => m.toLowerCase().includes(preferred));
        if (match) {
          selectedModel = match;
          break;
        }
      }

      this.model = selectedModel;
      logger.info(`Selected optimal LM Studio model: ${this.model}`);
    } catch (error) {
      logger.warn('Could not select optimal model, using first available');
    }
  }

  async healthCheck(): Promise<boolean> {
    return this.checkStatus();
  }

  supportsModel(_modelName: string): boolean {
    // LM Studio supports any model loaded in it
    return true;
  }

  getModelName(): string {
    return this.model;
  }
<<<<<<< HEAD

  /**
   * Estimate total tokens for context-aware timeout calculation
   */
  private estimateTokens(request: any): number {
    const promptText = request.prompt || '';
    const maxTokens = request.maxTokens || request.max_tokens || 16384;
    
    // Estimate input tokens (rough approximation: 1 token ≈ 4 characters)
    const estimatedInputTokens = Math.ceil(promptText.length / 4);
    
    // Add system prompt tokens (approximately 200 tokens for LM Studio)
    const systemPromptTokens = 200;
    
    return estimatedInputTokens + systemPromptTokens + maxTokens;
  }

  /**
   * Context-aware timeout based on token consumption and LM Studio performance
   * LM Studio typically runs optimized models and is faster than Ollama
   */
  private getContextAwareTimeout(request: any): number {
    const totalTokens = this.estimateTokens(request);
    
    // LM Studio processing speeds (tokens per second) - generally faster than Ollama
    const modelSpeeds = {
      // Small models (very fast)
      '1b': 120,   // ~120 tokens/sec
      '3b': 80,    // ~80 tokens/sec
      '7b': 40,    // ~40 tokens/sec
      // Medium models  
      '13b': 20,   // ~20 tokens/sec
      '14b': 18,   // ~18 tokens/sec
      // Large models
      '20b': 12,   // ~12 tokens/sec
      '30b': 8,    // ~8 tokens/sec
    };
    
    // Determine model speed based on model name
    let tokensPerSecond = 30; // Default fallback (faster than Ollama)
    for (const [size, speed] of Object.entries(modelSpeeds)) {
      if (this.model?.includes(size)) {
        tokensPerSecond = speed;
        break;
      }
    }
    
    // Calculate base processing time
    const baseProcessingTime = Math.ceil(totalTokens / tokensPerSecond) * 1000;
    
    // Add complexity factors (same as Ollama but LM Studio handles them better)
    let complexityMultiplier = 1;
    
    const promptText = request.prompt || '';
    
    // Tool usage increases complexity but LM Studio handles it well
    if (request.tools && request.tools.length > 0) {
      complexityMultiplier += 0.3; // Less impact than Ollama
    }
    
    // Complex reasoning tasks
    if (promptText.includes('audit') || promptText.includes('review') || 
        promptText.includes('analyze') || promptText.includes('debug')) {
      complexityMultiplier += 0.2; // Less impact due to optimization
    }
    
    // Code generation tasks are LM Studio's strength
    if (promptText.includes('generate') || promptText.includes('create') || 
        promptText.includes('write')) {
      complexityMultiplier -= 0.1; // Actually faster for code gen
    }
    
    // Ensure minimum multiplier
    complexityMultiplier = Math.max(complexityMultiplier, 0.7);
    
    // Add safety buffer (30% overhead - less than Ollama due to optimization)
    const finalTimeout = baseProcessingTime * complexityMultiplier * 1.3;
    
    // Reasonable bounds: minimum 5s, maximum 10 minutes (600s)
    const boundedTimeout = Math.max(5000, Math.min(finalTimeout, 600000));
    
    console.log(`🏛️ DEBUG: LM Studio context-aware timeout calculation:
      Total tokens: ${totalTokens}
      Model speed: ${tokensPerSecond} tokens/sec
      Base processing time: ${Math.round(baseProcessingTime/1000)}s
      Complexity multiplier: ${complexityMultiplier.toFixed(2)}
      Final timeout: ${Math.round(boundedTimeout/1000)}s`);
    
    return boundedTimeout;
  }
=======
>>>>>>> 312cb1b60a67735101a751485e0debd903886729
}
